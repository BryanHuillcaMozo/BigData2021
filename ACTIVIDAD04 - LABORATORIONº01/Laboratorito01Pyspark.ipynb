{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf0Oh9V_j0qa"
      },
      "source": [
        "TAREA: Laboratorio Nº 01\n",
        "----\n",
        "> **Asignatura:** Mineria de Datos \\\\\n",
        "> **Alumno :** Huillca Mozo, Bryan\\\n",
        "> **Codigo :** 160329 \\\\\n",
        "---\n",
        "\n",
        "\n",
        "![CMCC](https://scontent.fcuz1-1.fna.fbcdn.net/v/t1.6435-9/169999414_134997141968151_5389622841264143457_n.jpg?_nc_cat=100&ccb=1-5&_nc_sid=09cbfe&_nc_eui2=AeGYHrPMueJlavl7eDql2QOI2St2L7MLFHTZK3YvswsUdPzk3Ovp__UvRhUQlcFyH2o&_nc_ohc=KZJCq4jFuhMAX9h5HUI&_nc_ht=scontent.fcuz1-1.fna&oh=00_AT_sGFdClC9xMtR4S8p-ilZYOR4ycFv1uCTqoKj-frAi8g&oe=61F5FE4C)\n",
        "# **Spark + Python = PySpark**\n",
        "\n",
        "#### Este notebook presenta los conceptos básicos de Spark a través de su interfaz de lenguaje Python. Como aplicación inicial usaremos el ejemplo clásico del contador de palabras. Con este ejemplo, puede comprender la lógica de programación funcional para las diversas tareas de exploración de datos distribuidos.\n",
        "\n",
        "#### Para esto usaremos el libro de texto [Obras completas de William Shakespeare] (http://www.gutenberg.org/ebooks/100) obtenido del [Proyecto Gutenberg] (http://www.gutenberg.org/ wiki / Página_principal). Veremos que este mismo algoritmo se puede utilizar para textos de cualquier tamaño.\n",
        "\n",
        "#### **Este notebook contiene:**\n",
        "#### *Parte 1:* Creación de una base RDD y RDD de tupla\n",
        "#### *Parte 2:* Manejo de RDD de tuplas\n",
        "#### *Parte 3:* Encontrar palabras sueltas y calcular promedios\n",
        "#### *Parte 4:* Aplicar el recuento de palabras a un archivo\n",
        "#### *Parte 5:* Similitud entre objetos\n",
        "#### Para los ejercicios es recomendable consultar la documentación de [PySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Instalacion de dependencias para pyspark***"
      ],
      "metadata": {
        "id": "BnqLs5CQpJgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3CHc_uvpJ_I",
        "outputId": "46830fdc-846b-4ac0-a4b1-3e9bcf91d93d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,822 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.8 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [716 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,929 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,489 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,459 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.5 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [749 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,238 kB]\n",
            "Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:27 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.7 kB]\n",
            "Fetched 13.7 MB in 3s (3,998 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#libreria os para crear archivos temporales y directorios "
      ],
      "metadata": {
        "id": "HtZax-q0pS7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#Configuracion de variables JavaHome  y Spark_Home\n",
        "#java_Home\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# Spark_Home\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# usamos findspark para configurar Spark en nuestro cuaderno de colab\n",
        "#  o tambien jupiter si es el caso\n",
        "import findspark\n",
        "# inicializacion\n",
        "findspark.init()\n",
        "# Usamos PySpark\n",
        "import pyspark\n",
        "#Importamos SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate() \n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "9t_bDrMypR81",
        "outputId": "4dc7786c-9110-4fb4-9b03-693a24296519"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-2.3.1-bin-hadoop2.7\tspark-2.3.1-bin-hadoop2.7.tgz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8dd7f0a62a57:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f3c9bbfced0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "pip install pyspark "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "cellView": "form",
        "collapsed": true,
        "id": "NFbgYhO5pnGT",
        "outputId": "e9c1d3af-2869-4856-bbff-c98138b4b192"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 39 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 30.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=a3aaeb3bbe45f4c8ac45c15f37c956f3279296228f20e7df6816bf3e89393641\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "py4j",
                  "pyspark"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Permitir acceso a nuestro drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bGU7iR0pw-S",
        "outputId": "bbae7908-6bf9-4d97-9860-fe23ffea399d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Otras librerias usadas\n",
        "import os\n",
        "import pyspark\n",
        "# Importamos HasingTF: Asigna una secuencia de términos a sus \n",
        "# frecuencias de términos utilizando el truco de hash\n",
        "# IDF: Frecuencia inversa de documentos (IDF)\n",
        "# Tokenizer: convierte la cadena de entrada a minúsculas \n",
        "# y luego la divide por espacios en blanco.\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 150)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "knUqf86lp9L9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "sc = SparkSession\\\n",
        "        .builder\\\n",
        "        .appName(\"TfIdf Twitts\")\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CrSKyHZxp-EA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "sparkContext=spark.sparkContext"
      ],
      "metadata": {
        "id": "nO4G9AAUqUBW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indicar la version de pyspark a usar"
      ],
      "metadata": {
        "id": "8j306wy--RMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "_DdmjjJGsB-G",
        "outputId": "7c218f90-d117-4441-c866-14b73abf1283"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 28 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 58.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612246 sha256=1eda692f7a1c1c1ca9e697bdb52301c4275823b1a8f038afdf596354fa6d6a4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.2\n",
            "    Uninstalling py4j-0.10.9.2:\n",
            "      Successfully uninstalled py4j-0.10.9.2\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.2.0\n",
            "    Uninstalling pyspark-3.2.0:\n",
            "      Successfully uninstalled pyspark-3.2.0\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "py4j",
                  "pyspark"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importamos pysparkContext"
      ],
      "metadata": {
        "id": "bAWeidRP-b0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "aA9lQcM4sDVC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DObQ0zQj0qf"
      },
      "source": [
        "### **Parte 1: Creación y manipulación de RDD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BcaXQSVj0qg"
      },
      "source": [
        "#### En esta parte del notebook crearemos una base RDD a partir de una lista con el comando `parallelize`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bVnJT08j0qg"
      },
      "source": [
        "#### **(1a) Creando una base RDD**\n",
        "#### Podemos crear una base de datos RDD de diferentes tipos y fuente Python con el comando `sc.parallelize (fuente, particiones)`, siendo fuente una variable que contiene los datos (ej .: una lista) y particiona el número de particiones trabajar en paralelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0ZhA4RLSj0qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ee7264-72aa-4455-c079-7f20dd5766ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ],
      "source": [
        "ListaPalavras = ['gato', 'elefante', 'rato', 'rato', 'gato']\n",
        "\n",
        "palavrasRDD = sc.parallelize(ListaPalavras, 4)\n",
        "print(type(palavrasRDD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtRuS91Mj0qi"
      },
      "source": [
        "#### **(1b) Plural**\n",
        "\n",
        "#### Vamos a crear una función que transforme una palabra en plural agregando una letra 's' al final de la cadena. A continuación, usemos la función `map()` para aplicar la transformación a cada palabra del RDD.\n",
        "\n",
        "#### En Python (y muchos otros lenguajes) la concatenación de cadenas es costosa. Una mejor alternativa es crear una nueva cadena usando [`str.format()`] (https://docs.python.org/2/library/string.html#format-string-syntax).\n",
        "\n",
        "#### Nota: La cadena entre los conjuntos de tres comillas representa la documentación de la función. Esta documentación se muestra con el comando `help()`. Usaremos la estandarización de documentación sugerida para Python, mantendremos esta documentación en inglés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lYRt8a-ej0qj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1dfa41-4ffa-44a8-e050-89ca5c225c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gatos\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: f\"{palavra}s\"\n",
        "def Plural(palavra):\n",
        "    \"\"\"Adds an 's' to `palavra`.\n",
        "\n",
        "    Args:\n",
        "        palavra (str): A string.\n",
        "\n",
        "    Returns:\n",
        "        str: A string with 's' added to it.\n",
        "    \"\"\"\n",
        "    return f\"{palavra}s\"\n",
        "\n",
        "print(Plural('gato'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2j0IOJ70j0qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af43bc76-ba6f-44a8-876e-ec14a3ee37d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function Plural in module __main__:\n",
            "\n",
            "Plural(palavra)\n",
            "    Adds an 's' to `palavra`.\n",
            "    \n",
            "    Args:\n",
            "        palavra (str): A string.\n",
            "    \n",
            "    Returns:\n",
            "        str: A string with 's' added to it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(Plural)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "wO7LT9aYj0ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29c7e14-f63f-4e41-cce4-a72c5856c618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert Plural('rato')=='ratos', 'resultado incorreto!'\n",
        "print ('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biUwBPXPj0ql"
      },
      "source": [
        "#### **(1c) Aplicar la función a RDD**\n",
        "#### Pluralice cada palabra de nuestro RDD usando [map()] (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)\n",
        "\n",
        "#### A continuación, usaremos el comando [collect()] (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) que devuelve el RDD como una lista de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KjXvrnS9j0qm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e043bf62-58bc-411b-962c-769a9edfb6ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gatos', 'elefantes', 'ratos', 'ratos', 'gatos']\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: map(Plural)\n",
        "pluralRDD = palavrasRDD.map(Plural)\n",
        "print (pluralRDD.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "k7jkWLq4j0qm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8f3287-67d6-4391-ecbe-39ed1e5855a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert pluralRDD.collect()==['gatos','elefantes','ratos','ratos','gatos'], 'valores incorretos!'\n",
        "print ('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgDRZOB7j0qn"
      },
      "source": [
        "#### **Nota:** use el comando `collect()` solo cuando esté seguro de que la lista cabe en la memoria. Para guardar los resultados en un archivo de texto o base de datos, usaremos otro comando."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fnOJtfKj0qn"
      },
      "source": [
        "#### **(1d) Usando una función `lambda`**\n",
        "#### Repite la creación de un RDD plural, pero usando una función lambda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "8NkuY0a0j0qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123227a1-ae33-4c96-cf10-51f4662dd558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gatos', 'elefantes', 'ratos', 'ratos', 'gatos']\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: map(lambda x: f\"{x}s\")\n",
        "pluralLambdaRDD = palavrasRDD.map(lambda x: f\"{x}s\")\n",
        "print (pluralLambdaRDD.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "QAbUUOeZj0qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f9554e-da3c-4da1-d8b5-e0b135eea72f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert pluralLambdaRDD.collect()==['gatos','elefantes','ratos','ratos','gatos'], 'valores incorretos!'\n",
        "print ('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_2er89mj0qo"
      },
      "source": [
        "#### **(1e) Tamaño de cada palabra**\n",
        "#### Ahora usa `map()` y una función `lambda` para devolver el número de caracteres en cada palabra. Utilice `collect()` para almacenar el resultado como listas en la variable de destino."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xi0lVLlfj0qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bec05b2-d767-4f99-a367-ff7916bd6f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 9, 5, 5, 5]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: map(lambda x:len(x))\n",
        "pluralTamanho = (pluralRDD\n",
        "                 .map(lambda x:len(x))\n",
        "                 .collect()\n",
        "                 )\n",
        "print (pluralTamanho)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "aKy0UnYRj0qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4b7ed7-ed62-4fb7-e2f3-48899c50ab57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert pluralTamanho==[5,9,5,5,5], 'valores incorretos'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLJZyKArj0qp"
      },
      "source": [
        "#### **(1f) RDD de pares y tuplas**\n",
        "\n",
        "#### Para contar la frecuencia de cada palabra de forma distribuida, primero debemos asignar un valor a cada palabra en el RDD. Esto generará una base de datos (clave, valor). De esta forma podemos agrupar la base mediante la clave, calculando la suma de los valores asignados. En nuestro caso, asignemos el valor `1` a cada palabra.\n",
        "\n",
        "#### Un RDD que contiene la estructura de tupla clave-valor `(k, v)` se denomina RDD de tupla o * RDD de par *.\n",
        "\n",
        "#### Vamos a crear nuestro RDD a partir de pares usando la transformación `map ()` con una función `lambda ()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "RNuxp_pQj0qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93fb64d9-64f9-40f8-cf53-933fe5830df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('gato', 1), ('elefante', 1), ('rato', 1), ('rato', 1), ('gato', 1)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: map(lambda x:(x,1))\n",
        "\n",
        "palavraPar = palavrasRDD.map(lambda x:(x,1))\n",
        "print (palavraPar.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7E6WLL-kj0qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78d8644-4bc2-42d3-b987-0a404f957f5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert palavraPar.collect() == [('gato',1),('elefante',1),('rato',1),('rato',1),('gato',1)], 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJvMWB-yj0qq"
      },
      "source": [
        "### **Parte 2: Manipulación de Tuplas RDD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzd9HP6Lj0qq"
      },
      "source": [
        "#### Manipulemos nuestro RDD para contar las palabras en el texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ02Guzvj0qq"
      },
      "source": [
        "#### **(2a) Función `groupByKey ()`**\n",
        "\n",
        "#### La función [groupByKey ()] (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) agrupa todos los valores de un RDD por clave (primer elemento de la tupla) agregando los valores en una lista.\n",
        "\n",
        "#### Este enfoque tiene una debilidad porque:\n",
        "   + #### La operación requiere que los datos seccionados se muevan de forma masiva para que permanezcan en la partición correcta.\n",
        "   + #### Las listas pueden llegar a ser muy largas. Imagínese contar todas las palabras en Wikipedia: términos comunes como \"a\", \"e\" formarán una lista enorme de valores que podrían no caber en la memoria del proceso esclavo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "U_HUJ3EXj0qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a59d5b-9985-4c9c-9454-9cff03db9379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elefante: [1]\n",
            "rato: [1, 1]\n",
            "gato: [1, 1]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "palavrasGrupo = palavraPar.groupByKey()\n",
        "for chave, valor in palavrasGrupo.collect():\n",
        "    valores = list(valor)\n",
        "    print(f'{chave}: {valores}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Qk9fKNb8j0qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3160ce-afa0-40eb-b0de-fbd69677fb8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert sorted(palavrasGrupo.mapValues(lambda x: list(x)).collect()) == [('elefante', [1]), ('gato',[1, 1]), ('rato',[1, 1])], 'Valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojMAqft1j0qr"
      },
      "source": [
        "#### **(2b) Cálculo de los recuentos**\n",
        "#### Después de `groupByKey()` nuestro RDD contiene elementos compuestos por la palabra, como clave, y un iterador que contiene todos los valores correspondientes a esa clave.\n",
        "#### Usando la transformación `map()` y la función `sum()`, construye un nuevo RDD que consta de tuplas (clave, suma)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "mf44Jnufj0qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e0929d-0adf-41b2-ac8d-e20072589645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: map(lambda x:(x[0],sum(x[1])))\n",
        "contagemGroup = palavrasGrupo.map(lambda x:(x[0],sum(x[1])))\n",
        "print (contagemGroup.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Ehx5sI4Bj0qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe9115f-3d04-42ed-d3f9-0ffc99901fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert sorted(contagemGroup.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bfxPudFj0qs"
      },
      "source": [
        "#### **(2c) `reduceByKey`**\n",
        "#### Un comando más interesante para contar es [reduceByKey ()] (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) que crea un nuevo RDD de tuplas.\n",
        "\n",
        "#### Esta transformación aplica la transformación `reduce()` vista en la lección anterior a los valores de cada clave. De esta manera, la función de transformación puede aplicarse a cada partición local y luego enviarse para la redistribución de la partición, reduciendo la cantidad de datos que se mueven y no manteniendo grandes listas en la memoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "VwXwzRjgj0qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef344f53-fc0e-4abf-ea1c-0ff09b3658cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: reduceByKey(lambda x,y:x+y)\n",
        "contagem = palavraPar.reduceByKey(lambda x,y:x+y)\n",
        "print( contagem.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "2bO6Sh2ij0qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59090ab7-9401-47ed-b573-8499acbe35c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert sorted(contagem.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th4Hx-9Lj0qt"
      },
      "source": [
        "#### **(2d) Comandos de agrupación**\n",
        "\n",
        "#### La forma más común de realizar esta tarea, a partir de nuestras RDD RDDwords, es encadenar el mapa y los comandos reduceByKey en una línea de comando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "VwnRqrP9j0qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53acbb4d-4332-4bdf-a4d6-68da65bf2454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: map(lambda x:(x,1))  \n",
        "# COMPLETAR: reduceByKey(lambda x,y:x+y)\n",
        "contagemFinal = (palavrasRDD\n",
        "                 .map(lambda x:(x,1))  \n",
        "                 .reduceByKey(lambda x,y:x+y)                \n",
        "                 )\n",
        "print (contagemFinal.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "gXUnGTr-j0qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b62599-b603-44d5-a3fa-d7669090022c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert sorted(contagemFinal.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL8gFPxDj0qu"
      },
      "source": [
        "### **Parte 3: Encontrar las palabras únicas y calcular el recuento promedio**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJYUMqtwj0qu"
      },
      "source": [
        "#### **(3a) Palabras únicas**\n",
        "\n",
        "#### Calcula el número de palabras únicas en RDD. Utilice la misma canalización que antes pero reemplazando `.collect()` con otro método de API RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "11JMko13j0qu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72002856-f9dc-451b-cbe8-8af4efca3512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAAR: .count()\n",
        "palavrasUnicas = (contagemFinal\n",
        "                  .count()\n",
        "                 )\n",
        "print (palavrasUnicas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "v6tJKGJrj0qu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aadb998-bf6d-4228-87f0-c1dd8c98b645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert palavrasUnicas==3, 'valor incorreto!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwRt3f5Cj0qv"
      },
      "source": [
        "#### **(3b) Cálculo del recuento promedio de palabras**\n",
        "\n",
        "#### Encuentra la frecuencia promedio de palabras usando RDD `count`.\n",
        "\n",
        "#### Tenga en cuenta que la función de comando `reduce()` se aplica a cada tupla de RDD. Para realizar la suma de los recuentos, primero es necesario mapear el RDD a un RDD que contenga solo los valores de frecuencia (sin las claves)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rtSryNe4j0qv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3850ce5-3780-4e7c-99f4-56bd47c0a992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "1.67\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# add é equivalente a lambda x,y: x+y\n",
        "# COMPLETAR: map(lambda x: x[1])\n",
        "# COMPLETAR  reduce(add))\n",
        "from operator import add\n",
        "total = (contagemFinal\n",
        "         .map(lambda x: x[1])\n",
        "         .reduce(add)\n",
        "         )\n",
        "media = total / float(palavrasUnicas)\n",
        "print (total)\n",
        "print (round(media, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "UbVYm7cgj0qv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f77f48e-58eb-45eb-9b50-3b9106a6327f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert round(media, 2)==1.67, 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMV4vjcpj0qv"
      },
      "source": [
        "### **Parte 4: Aplicar nuestro algoritmo a un archivo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EHpsdQ4j0qv"
      },
      "source": [
        "#### ** (4a) Función `contaPalavras` **\n",
        "\n",
        "#### Para aplicar nuestro algoritmo genéricamente a múltiples RDD, primero creemos una función para aplicarla a cualquier fuente de datos. Esta función recibe la entrada de un RDD que contiene una lista de claves (palabras) y devuelve un RDD de tuplas con las claves y su recuento en ese RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "PXNCIubIj0qw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef44d075-b48b-4ff7-e737-8067f931f14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: map(lambda x:(x,1))  \n",
        "# COMPLETAR: reduceByKey(lambda x,y:x+y)\n",
        "def contaPalavras(chavesRDD):\n",
        "    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n",
        "\n",
        "    Args:\n",
        "        chavesRDD (RDD of str): An RDD consisting of words.\n",
        "\n",
        "    Returns:\n",
        "        RDD of (str, int): An RDD consisting of (word, count) tuples.\n",
        "    \"\"\"\n",
        "    return (chavesRDD\n",
        "            .map(lambda x:(x,1))  \n",
        "            .reduceByKey(lambda x,y:x+y)\n",
        "           )\n",
        "\n",
        "print (contaPalavras(palavrasRDD).collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "u8AT1J1pj0qw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7ece4e-488b-41e4-a406-4ece69dac89f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert sorted(contaPalavras(palavrasRDD).collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zHtq-2Pj0qw"
      },
      "source": [
        "#### ** (4b) Normalizando el texto **\n",
        "\n",
        "#### Cuando trabajamos con datos reales, normalmente necesitamos estandarizar los atributos de tal manera que se ignoren las diferencias sutiles debidas a errores de medición o diferencias de estandarización. Para el siguiente paso, estandarizaremos el texto para:\n",
        "   + #### Estandarizar las mayúsculas de las palabras (todas en mayúsculas o todas en minúsculas).\n",
        "   + #### Eliminar puntuación.\n",
        "   + #### Eliminar espacios al principio y al final de la palabra.\n",
        " \n",
        "#### Crea una función `removerPontuacao` que convierte todo el texto a minúsculas, elimina los signos de puntuación y los espacios en blanco al principio o al final de la palabra. Para esto, use la biblioteca [re] (https://docs.python.org/2/library/re.html) para eliminar todo el texto que no sea letra, número o espacio, enlazando con las funciones de cadena para eliminar los espacios en blanco y convertir a minúsculas (consulte [Cadenas] (https://docs.python.org/2/library/stdtypes.html?highlight=str.lower#string-methods))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "uqQq784bj0qw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e8a5b0c-0c7a-42b6-dc43-b0fefc61d287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ola quem esta ai\n",
            "sem espaco esublinhado\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "import re\n",
        "def removerPontuacao(texto):\n",
        "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
        "\n",
        "    Note:\n",
        "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
        "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
        "        punctuation is removed.\n",
        "\n",
        "    Args:\n",
        "        texto (str): A string.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned up string.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^A-Za-z0-9 ]', '', texto).strip().lower()\n",
        "print (removerPontuacao('Ola, quem esta ai??!'))\n",
        "print (removerPontuacao(' Sem espaco e_sublinhado!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "_hGRkGUuj0qx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea2b4cf-c6d4-43a3-c03d-dca430b6bb0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert removerPontuacao(' O uso de virgulas, embora permitido, nao deve contar. ')=='o uso de virgulas embora permitido nao deve contar', 'string incorreta!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcnxwWr2j0qx"
      },
      "source": [
        "#### **(4c) Cargando archivo de texto**\n",
        "\n",
        "#### Para la siguiente parte usaremos el libro [Obras completas de William Shakespeare] (http://www.gutenberg.org/ebooks/100) de [Proyecto Gutenberg] (http://www.gutenberg.org / wiki / Página_principal).\n",
        "\n",
        "#### Para convertir un texto en un RDD, usamos la función `textFile()` que toma como entrada el nombre del archivo de texto que queremos usar y el número de particiones.\n",
        "\n",
        "#### El nombre del archivo de texto puede hacer referencia a un archivo local o un URI de archivo distribuido (por ejemplo, hdfs: //).\n",
        "\n",
        "#### Apliquemos también la función `removeScore()` para normalizar el texto y verifiquemos las primeras 15 líneas con el comando `take()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Z8QgpqJEj0qx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9274a88-ffa9-4f58-8116-0c6f59553b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": 0\n",
            "project gutenbergs the complete works of william shakespeare by william: 1\n",
            "shakespeare: 2\n",
            ": 3\n",
            "this ebook is for the use of anyone anywhere in the united states and: 4\n",
            "most other parts of the world at no cost and with almost no restrictions: 5\n",
            "whatsoever  you may copy it give it away or reuse it under the terms: 6\n",
            "of the project gutenberg license included with this ebook or online at: 7\n",
            "wwwgutenbergorg  if you are not located in the united states youll: 8\n",
            "have to check the laws of the country where you are located before using: 9\n",
            "this ebook: 10\n",
            ": 11\n",
            "see at the end of this file  content note added in 2017: 12\n",
            ": 13\n",
            ": 14\n"
          ]
        }
      ],
      "source": [
        "# Apenas execute a célula\n",
        "import os.path\n",
        "\n",
        "arquivo = os.path.join('pg100.txt') \n",
        "\n",
        "# lê o arquivo com textFile e aplica a função removerPontuacao        \n",
        "shakesRDD = (sc\n",
        "             .textFile(arquivo, 8)\n",
        "             .map(removerPontuacao)\n",
        "             )\n",
        "\n",
        "# zipWithIndex gera tuplas (conteudo, indice) onde indice é a posição do conteudo na lista sequencial\n",
        "# Ex.: sc.parallelize(['gato','cachorro','boi']).zipWithIndex() ==> [('gato',0), ('cachorro',1), ('boi',2)]\n",
        "# sep.join() junta as strings de uma lista através do separador sep. Ex.: ','.join(['a','b','c']) ==> 'a,b,c'\n",
        "print ('\\n'.join(shakesRDD\n",
        "                .zipWithIndex()\n",
        "                .map(lambda linha: '{0}: {1}'.format(linha[0],linha[1]))\n",
        "                .take(15)\n",
        "               ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g7kiLmcj0qx"
      },
      "source": [
        "#### **(4d) Extrayendo las palabras**\n",
        "#### Antes de que podamos usar nuestra función `contaPalavras()`, todavía tenemos que trabajar sobre nuestro RDD:\n",
        "   + #### Necesitamos generar listas de palabras en lugar de listas de oraciones.\n",
        "   + #### Eliminar líneas vacías.\n",
        " \n",
        "#### Las cadenas de Python tienen el método [split ()] (https://docs.python.org/2/library/string.html#string.split) que separa una cadena por un separador. En nuestro caso, queremos separar las cadenas por espacio.\n",
        "\n",
        "#### Usa la función `map()` para generar un nuevo RDD como lista de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "N3kAMgtxj0qx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54cd6b1-72ab-4b66-8923-cc11a89d1870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[], ['project', 'gutenbergs', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'by', 'william'], ['shakespeare'], [], ['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and']]\n",
            "147929\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: map(lambda x:x.split())\n",
        "shakesPalavrasRDD = shakesRDD.map(lambda x:x.split())\n",
        "total = shakesPalavrasRDD.count()\n",
        "print (shakesPalavrasRDD.take(5))\n",
        "print (total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaJr-LIuj0qy"
      },
      "source": [
        "#### Como habrás notado, el uso de la función `map()` genera una lista para cada línea, creando un RDD que contiene una lista de listas.\n",
        "\n",
        "#### Para resolver este problema, Spark tiene una función análoga llamada `flatMap ()` que aplica la transformación `map()`, pero *aplana* el retorno en forma de lista a una lista unidimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "y8Kj6I40j0qy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f158b1ff-c98c-416a-adf9-d9e9cc74bbe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['project', 'gutenbergs', 'the', 'complete', 'works']\n",
            "959359\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "shakesPalavrasRDD = shakesRDD.flatMap(lambda x: x.split())\n",
        "total = shakesPalavrasRDD.count()\n",
        "print (shakesPalavrasRDD.take(5))\n",
        "print (total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "lrot402Pj0qy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4555d8f0-3fba-4d10-f838-5bd68638a422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert total==959359, \"valor incorreto de palavras!\"\n",
        "print (\"OK\")\n",
        "assert shakesPalavrasRDD.take(5)==['project', 'gutenbergs', 'the', 'complete', 'works'],'lista incorreta de palavras'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiPCoeG-j0qy"
      },
      "source": [
        "#### Tenga en cuenta que `flatMap` ya ha eliminado las entradas vacías."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wENX1ehj0qz"
      },
      "source": [
        "#### ** (4f) Número de palabras **\n",
        "#### Ahora que nuestro RDD contiene una lista de palabras, podemos aplicar nuestra función `contaPalavras()`.\n",
        "\n",
        "#### Aplica la función a nuestro RDD y usa la función `takeOrdered` para imprimir las 15 palabras más frecuentes.\n",
        "\n",
        "#### `takeOrdered()` puede tomar un segundo parámetro que le dice a Spark cómo ordenar los elementos. Ex.:\n",
        "\n",
        "#### `takeOrdered (15, key=lambda x: -x)`: orden descendente de valores x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "jJAozs0Zj0qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b8deff-2cb0-415b-b65e-5e75b0017cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the: 29996\n",
            "and: 28353\n",
            "i: 21860\n",
            "to: 20885\n",
            "of: 18811\n",
            "a: 15992\n",
            "you: 14439\n",
            "my: 13191\n",
            "in: 12027\n",
            "that: 11782\n",
            "is: 9711\n",
            "not: 9068\n",
            "with: 8521\n",
            "me: 8271\n",
            "for: 8184\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# COMPLETAR: contaPalavras(shakesPalavrasRDD).takeOrdered(15, key=lambda x : -x[1])\n",
        "top15 = contaPalavras(shakesPalavrasRDD).takeOrdered(15, key=lambda x : -x[1])\n",
        "print ('\\n'.join(map(lambda x: f'{x[0]}: {x[1]}', top15)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "d9bqivEwj0qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2498e6-39d0-4a4f-a2b5-65940b263ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert top15 == [('the', 29996), ('and', 28353), ('i', 21860), ('to', 20885), ('of', 18811), ('a', 15992), ('you', 14439), ('my', 13191), ('in', 12027), ('that', 11782), ('is', 9711), ('not', 9068), ('with', 8521), ('me', 8271), ('for', 8184)],'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY6OssFuj0qz"
      },
      "source": [
        "### **Parte 5: Similaridad entre Objetos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJXkyrGyj0qz"
      },
      "source": [
        "### En esta parte del laboratorio aprenderemos a calcular la distancia entre atributos numéricos, categóricos y textuales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4N0UUv5j0qz"
      },
      "source": [
        "#### ** (5a) Vectores en el espacio euclidiano **\n",
        "\n",
        "#### Cuando nuestros objetos están representados en el espacio euclidiano, medimos la similitud entre ellos a través de la *p-Norma* definida por:\n",
        "\n",
        "#### $$d(x,y,p) = (\\sum_{i=1}^{n}{|x_i - y_i|^p})^{1/p}$$\n",
        "\n",
        "#### Las normas más utilizadas son $p=1,2,\\infty$ que reducen en distancia absoluta, Euclidiana y distancia máxima:\n",
        "\n",
        "#### $$d(x,y,1) = \\sum_{i=1}^{n}{|x_i - y_i|}$$\n",
        "\n",
        "#### $$d(x,y,2) = (\\sum_{i=1}^{n}{|x_i - y_i|^2})^{1/2}$$\n",
        "\n",
        "#### $$d(x,y,\\infty) = \\max(|x_1 - y_1|,|x_2 - y_2|, ..., |x_n - y_n|)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "INc4Kmj2j0q0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Vamos criar uma função pNorm que recebe como parâmetro p e retorna uma função que calcula a pNorma\n",
        "def pNorm(p):\n",
        "    \"\"\"Generates a function to calculate the p-Norm between two points.\n",
        "\n",
        "    Args:\n",
        "        p (int): The integer p.\n",
        "\n",
        "    Returns:\n",
        "        Dist: A function that calculates the p-Norm.\n",
        "    \"\"\"\n",
        "\n",
        "    def Dist(x,y):\n",
        "        return np.power(np.power(np.abs(x-y),p).sum(),1/float(p))\n",
        "    return Dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "ibl1bcSbj0q0"
      },
      "outputs": [],
      "source": [
        "# Vamos criar uma RDD com valores numéricos\n",
        "np.random.seed(42)\n",
        "numPointsRDD = sc.parallelize(enumerate(np.random.random(size=(10,100))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "-stAApgaj0q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297efb86-666a-4eee-d313-bd12952564c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 4.709048183663605 3.7511916889753705\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
        "\n",
        "# COMPLETAR: cartesian(numPointsRDD)\n",
        "cartPointsRDD = numPointsRDD.cartesian(numPointsRDD)\n",
        "\n",
        "# Aplique um mapa para transformar nossa RDD em uma RDD de tuplas ((id1,id2), (vetor1,vetor2))\n",
        "# DICA: primeiro utilize o comando take(1) e imprima o resultado para verificar o formato atual da RDD\n",
        "\n",
        "#COMPLETAR: map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "cartPointsParesRDD = cartPointsRDD.map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "\n",
        "\n",
        "\n",
        "# Aplique um mapa para calcular a Distância Euclidiana entre os pares\n",
        "Euclid = pNorm(2)\n",
        "\n",
        "#COMPLETAR: map(lambda x: (x[0], Euclid(x[1][0],x[1][1])))\n",
        "distRDD = cartPointsParesRDD.map(lambda x: (x[0], Euclid(x[1][0],x[1][1])))\n",
        "\n",
        "# Encontre a distância máxima, mínima e média, aplicando um mapa que transforma (chave,valor) --> valor\n",
        "# e utilizando os comandos internos do pyspark para o cálculo da min, max, mean\n",
        "\n",
        "#COMPLETAR: distRDD.map(lambda x: x[1])\n",
        "statRDD = distRDD.map(lambda x: x[1])\n",
        "\n",
        "#COMPLETAR:min()\n",
        "#COMPLETAR:max()\n",
        "#COMPLETAR:mean()\n",
        "minv, maxv, meanv = statRDD.min(), statRDD.max(), statRDD.mean()\n",
        "print (minv, maxv, meanv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "ZRncQkBQj0q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7bb634-ff16-4955-ccad-2b248448051b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert (minv.round(2), maxv.round(2), meanv.round(2))==(0.0, 4.71, 3.75), 'Valores incorretos'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHSSa6a1j0q0"
      },
      "source": [
        "#### ** (5b) Valores Categóricos **\n",
        "\n",
        "#### Cuando nuestros objetos están representados por atributos categóricos, no tienen similitud espacial. Para calcular la similitud entre ellos podemos primero transformar nuestro vector de atributos en un vector binario indicando, para cada valor posible de cada atributo, si tiene ese atributo o no.\n",
        "\n",
        "#### Con el vector binario podemos usar la distancia de Hamming definida por:\n",
        "\n",
        "#### $$ H(x,y) = \\sum_{i=1}^{n}{x_i != y_i} $$\n",
        "\n",
        "#### También puedes establecer la distancia de Jaccard como:\n",
        "\n",
        "#### $$ J(x,y) = \\frac{\\sum_{i=1}^{n}{x_i == y_i} }{\\sum_{i=1}^{n}{\\max(x_i, y_i}) } $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "1TAc7cMAj0q1"
      },
      "outputs": [],
      "source": [
        "# Vamos criar uma função para calcular a distância de Hamming\n",
        "def Hamming(x,y):\n",
        "    \"\"\"Calculates the Hamming distance between two binary vectors.\n",
        "\n",
        "    Args:\n",
        "        x, y (np.array): Array of binary integers x and y.\n",
        "\n",
        "    Returns:\n",
        "        H (int): The Hamming distance between x and y.\n",
        "    \"\"\"\n",
        "    return (x!=y).sum()\n",
        "\n",
        "# Vamos criar uma função para calcular a distância de Jaccard\n",
        "def Jaccard(x,y):\n",
        "    \"\"\"Calculates the Jaccard distance between two binary vectors.\n",
        "\n",
        "    Args:\n",
        "        x, y (np.array): Array of binary integers x and y.\n",
        "\n",
        "    Returns:\n",
        "        J (int): The Jaccard distance between x and y.\n",
        "    \"\"\"\n",
        "    return (x==y).sum()/float( np.maximum(x,y).sum() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "Q7vjYkevj0q1"
      },
      "outputs": [],
      "source": [
        "# Vamos criar uma RDD com valores categóricos\n",
        "catPointsRDD = sc.parallelize(enumerate([['alto', 'caro', 'azul'],\n",
        "                             ['medio', 'caro', 'verde'],\n",
        "                             ['alto', 'barato', 'azul'],\n",
        "                             ['medio', 'caro', 'vermelho'],\n",
        "                             ['baixo', 'barato', 'verde'],\n",
        "                            ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ctVjR7b9j0q1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fec69d3-212f-47ff-ea25-cdf6e3e79897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 'alto'), (1, 'caro'), (2, 'alto'), (3, 'caro'), (4, 'baixo'), (4, 'verde'), (0, 'caro'), (0, 'azul'), (1, 'medio'), (1, 'verde'), (2, 'barato'), (2, 'azul'), (3, 'medio'), (3, 'vermelho'), (4, 'barato')]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# Crie um RDD de chaves únicas utilizando flatMap\n",
        "\n",
        "# COMPLETAR: flatMap(lambda x: [((x[0],xi),1) for xi in x[1]])\n",
        "# COMPLETAR: map(lambda x: x[0])\n",
        "# COMPLETAR: reduceByKey(lambda x,y: x)\n",
        "chavesRDD = (catPointsRDD\n",
        "             .flatMap(lambda x: [((x[0],xi),1) for xi in x[1]])\n",
        "             .reduceByKey(lambda x,y: x)\n",
        "             .map(lambda x: x[0])\n",
        "             )\n",
        "print(chavesRDD.collect())\n",
        "chaves = dict((v,k) for k,v in chavesRDD.collect())\n",
        "nchaves = len(chaves)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (chaves, nchaves)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYu60vHl5X2g",
        "outputId": "0e219fdf-f52a-4772-83af-ff41fd23ab0d"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'alto': 2, 'caro': 0, 'baixo': 4, 'verde': 1, 'azul': 2, 'medio': 3, 'barato': 4, 'vermelho': 3} 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "PZM14jwVj0q1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca37a1fb-c89b-45c6-85e9-ec835410906b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert chaves=={'alto': 2, 'caro': 0, 'baixo': 4, 'verde': 1, 'azul': 2, 'medio': 3, 'barato': 4, 'vermelho': 3}, 'valores incorretos!'\n",
        "print (\"OK\")\n",
        "\n",
        "assert nchaves==8, 'número de chaves incorreta'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "995Q_NFRj0q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83503795-74f5-4df9-898c-e8a63a4ef27f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, array([1., 0., 1., 0., 0., 0., 0., 0.])),\n",
              " (1, array([1., 1., 0., 1., 0., 0., 0., 0.])),\n",
              " (2, array([0., 0., 1., 0., 1., 0., 0., 0.])),\n",
              " (3, array([1., 0., 0., 1., 0., 0., 0., 0.])),\n",
              " (4, array([0., 1., 0., 0., 1., 0., 0., 0.]))]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "def CreateNP(atributos,chaves):  \n",
        "    \"\"\"Binarize the categorical vector using a dictionary of keys.\n",
        "\n",
        "    Args:\n",
        "        atributos (list): List of attributes of a given object.\n",
        "        chaves (dict): dictionary with the relation attribute -> index\n",
        "\n",
        "    Returns:\n",
        "        array (np.array): Binary array of attributes.\n",
        "    \"\"\"\n",
        "    \n",
        "    array = np.zeros(len(chaves))\n",
        "    for atr in atributos:\n",
        "        array[ chaves[atr] ] = 1\n",
        "    return array\n",
        "\n",
        "# Converte o RDD para o formato binário, utilizando o dict chaves\n",
        "binRDD = catPointsRDD.map(lambda rec: (rec[0],CreateNP(rec[1], chaves)))\n",
        "binRDD.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "9pXbK-3hj0q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53da92c9-d57a-4794-8bb8-b2f4fe294237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\tMin\tMax\tMean\n",
            "Hamming:\t0.00\t5.00\t2.40\n",
            "Jaccard:\t0.60\t4.00\t1.90\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
        "\n",
        "#COMPLETAR: cartesian(binRDD)\n",
        "cartBinRDD = binRDD.cartesian(binRDD)\n",
        "\n",
        "# Aplique um mapa para transformar nossa RDD em uma RDD de tuplas ((id1,id2), (vetor1,vetor2))\n",
        "# DICA: primeiro utilize o comando take(1) e imprima o resultado para verificar o formato atual da RDD\n",
        "\n",
        "#COMPLETAR: map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "cartBinParesRDD = cartBinRDD.map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "\n",
        "\n",
        "# Aplique um mapa para calcular a Distância de Hamming e Jaccard entre os pares\n",
        "\n",
        "#COMPLETAR: map(lambda x: (x[0], Hamming(x[1][0],x[1][1])))\n",
        "#COMPLETAR: map(lambda x: (x[0], Jaccard(x[1][0],x[1][1])))\n",
        "hamRDD = cartBinParesRDD.map(lambda x: (x[0], Hamming(x[1][0],x[1][1])))\n",
        "jacRDD = cartBinParesRDD.map(lambda x: (x[0], Jaccard(x[1][0],x[1][1])))\n",
        "\n",
        "# Encontre a distância máxima, mínima e média, aplicando um mapa que transforma (chave,valor) --> valor\n",
        "# e utilizando os comandos internos do pyspark para o cálculo da min, max, mean\n",
        "\n",
        "#COMPLETAR: map(lambda x: x[1])\n",
        "#COMPLETAR: map(lambda x: x[1])\n",
        "statHRDD = hamRDD.map(lambda x: x[1])\n",
        "statJRDD = jacRDD.map(lambda x: x[1])\n",
        "\n",
        "#COMPLETAR: min()\n",
        "#COMPLETAR: max()\n",
        "#COMPLETAR: mean()\n",
        "Hmin, Hmax, Hmean = statHRDD.min(), statHRDD.max(), statHRDD.mean()\n",
        "\n",
        "#COMPLETAR: min()\n",
        "#COMPLETAR: max()\n",
        "#COMPLETAR: mean()\n",
        "Jmin, Jmax, Jmean = statJRDD.min(), statJRDD.max(), statJRDD.mean()\n",
        "\n",
        "print (\"\\t\\tMin\\tMax\\tMean\")\n",
        "print (\"Hamming:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(Hmin, Hmax, Hmean ))\n",
        "print (\"Jaccard:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format( Jmin, Jmax, Jmean ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "7uhrIUI6j0q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e42858-6370-4ff9-b6f1-668b909b7f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert (Hmin.round(2), Hmax.round(2), Hmean.round(2)) == (0.00,5.00,2.40), 'valores incorretos'\n",
        "print (\"OK\")\n",
        "assert (Jmin.round(2), Jmax.round(2), Jmean.round(2)) == (0.60,4.00,1.90), 'valores incorretos'\n",
        "print (\"OK\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Laboratorito01Pyspark.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}