{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocesamiento.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#   **EJERCICIOS PRE-PROCESAMIENTO**\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Alumno: Huillca Mozo, Bryan\n",
        "# Codigo: 160329"
      ],
      "metadata": {
        "id": "0kZdv5QWfWJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Instalacion de dependencias para pyspark***"
      ],
      "metadata": {
        "id": "_Ot4EaYP1K8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFgQpbgefXJE",
        "outputId": "34998d16-b801-4e1a-cb9f-2f5b1a8751e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Get:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,822 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,459 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.8 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [716 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,489 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,929 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,238 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [749 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.5 kB]\n",
            "Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:27 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.7 kB]\n",
            "Fetched 13.7 MB in 3s (3,945 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#libreria os para crear archivos temporales y directorios mira el módulo tempfile"
      ],
      "metadata": {
        "id": "TY6L-BDh1Ut8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#Configuracion de variables JavaHome  y Spark_Home\n",
        "#java_Home\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# Spark_Home\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# usamos findspark para configurar Spark en nuestro cuaderno de colab\n",
        "#  o tambien jupiter si es el caso\n",
        "import findspark\n",
        "# inicializacion\n",
        "findspark.init()\n",
        "# Usamos PySpark\n",
        "import pyspark\n",
        "#Importamos SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate() \n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "MLs2AuwXfdmq",
        "outputId": "220c4f01-d7e9-4a93-f670-4589e5429064"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-2.3.1-bin-hadoop2.7\tspark-2.3.1-bin-hadoop2.7.tgz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://1f5b7b8381ee:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe1c704a150>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Permitir acceso a nuestro drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxNh_ADsfqfW",
        "outputId": "f7de924b-cd1c-41a9-d047-bdbb0dd216de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Otras librerias usadas\n",
        "import os\n",
        "import pyspark\n",
        "# Importamos HasingTF: Asigna una secuencia de términos a sus \n",
        "# frecuencias de términos utilizando el truco de hash\n",
        "# IDF: Frecuencia inversa de documentos (IDF)\n",
        "# Tokenizer: convierte la cadena de entrada a minúsculas \n",
        "# y luego la divide por espacios en blanco.\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 150)"
      ],
      "metadata": {
        "id": "MMf402Bzf0ov",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indicamos la version de Pyspark a usar"
      ],
      "metadata": {
        "id": "ZmjXhmXS3f_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "z5YAuITlR3dr",
        "outputId": "ae68446f-9c40-4839-99cf-fa9e965f4d17"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 57.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612246 sha256=dcd98c048fb87e95874ae26def2a0d3210d30b638baf2fd0c72066c41acd1799\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "py4j",
                  "pyspark"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importamos SparkContext"
      ],
      "metadata": {
        "id": "q-JDu-WEtKKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "import math"
      ],
      "metadata": {
        "id": "uMCvPueKR3n8"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zTQCZLUStS3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "  # **TRABAJO PYSPARK: CINCO ALGORITMOS DE PRE-PROCESAMIENTO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "V6eY9UuzjlbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) NORMALIZACION\n",
        "---\n"
      ],
      "metadata": {
        "id": "gF93wiE0haA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizar elementos de un RDD\n",
        "\n",
        "\n",
        "1.   Calculamos la normal para el df que sera un array de numeros.\n",
        "2.   Normalizacion con map y lo almacenamos en value1.\n",
        "3.   A esta la guardamos en otra value2 para llamar a la funcion sum() en value.\n",
        "4.   Realizamos el proceso de calculo al df con map\n",
        "5.   devolvemos el df normalizado\n",
        "\n"
      ],
      "metadata": {
        "id": "Qe0C08jdhzna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Normalizacion(df):\n",
        "  # A*A\n",
        "  value = df.map(lambda A:A**2)\n",
        "  value2 = value.sum()\n",
        "  N = value2**0.5\n",
        "  #PROCESO\n",
        "  Process = df.map(lambda A: A/N)\n",
        "  return Process"
      ],
      "metadata": {
        "id": "2yP7AffjhaR8"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Generacion de df mediante arreglo de numeros pares\n",
        "2.   Paralelizacion RDD con 4 particiones\n",
        "3.   llamamos a nuestra funcion y la guardamos en una variable valFinal\n"
      ],
      "metadata": {
        "id": "CbFveOwHikbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=[2,4,6,8,10,12,14,16,18,20,22]\n",
        "Array2 = sc.parallelize(df,4)\n",
        "valFinal = Normalizacion(Array2)"
      ],
      "metadata": {
        "id": "T6BmGdbqheUo"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coleccion de elementos normalizados"
      ],
      "metadata": {
        "id": "dy4wwdIzisii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valFinal.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIhkgYCRhjAJ",
        "outputId": "1918bb7e-924e-422a-b94d-20fe5f7ff80c"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.044455422447438706,\n",
              " 0.08891084489487741,\n",
              " 0.1333662673423161,\n",
              " 0.17782168978975482,\n",
              " 0.2222771122371935,\n",
              " 0.2667325346846322,\n",
              " 0.3111879571320709,\n",
              " 0.35564337957950964,\n",
              " 0.40009880202694836,\n",
              " 0.444554224474387,\n",
              " 0.48900964692182575]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2) ESCALONAMIENTO**\n",
        "---"
      ],
      "metadata": {
        "id": "La5NPA81koBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escalonamiento: elementos de un RDD\n",
        "\n",
        "\n",
        "1.   Calculamos el minimo y maximo elemento del df\n",
        "2.   Normalizacion con map y lo almacenamos en value1.\n",
        "3.   Escalamiento: Realizamos el proceso de calculo al df con map\n",
        "4.   devolvemos el df Escalonado"
      ],
      "metadata": {
        "id": "UG12l938rSIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Escalonamiento(df):\n",
        "  #MAXIMO\n",
        "  MaxDf = df.max()\n",
        "  #MINIMO\n",
        "  MinDf = df.min()\n",
        "  #PROCESO DE ESCALONAMIENTO\n",
        "  Process = df.map(lambda A: ((A-MinDf)/(MaxDf-MinDf)))\n",
        "  return Process\n"
      ],
      "metadata": {
        "id": "_Kv6uMPCo6W-"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Generacion de df mediante arreglo de numeros pares\n",
        "2.   Paralelizacion RDD con 4 particiones\n",
        "3.   llamamos a nuestra funcion y la guardamos en una variable valFinal2"
      ],
      "metadata": {
        "id": "orkKC-_TsAxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=[2,4,6,8,10,12,14,16,18,20,22]\n",
        "Array2 = sc.parallelize(df,4)\n",
        "valFinal2 = Escalonamiento(Array2)"
      ],
      "metadata": {
        "id": "fssx95nEqiGK"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coleccion de datos escalonados"
      ],
      "metadata": {
        "id": "E76GfDOzsMfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valFinal.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDfISqeMpz7I",
        "outputId": "845338d4-bad8-49c7-b822-19437a90b31f"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.044455422447438706,\n",
              " 0.08891084489487741,\n",
              " 0.1333662673423161,\n",
              " 0.17782168978975482,\n",
              " 0.2222771122371935,\n",
              " 0.2667325346846322,\n",
              " 0.3111879571320709,\n",
              " 0.35564337957950964,\n",
              " 0.40009880202694836,\n",
              " 0.444554224474387,\n",
              " 0.48900964692182575]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3) ESTANDARIZACION\n",
        "---\n"
      ],
      "metadata": {
        "id": "LiLIYQtcWhPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ESTANDARIZAR ELEMENTOS DE UN RDD"
      ],
      "metadata": {
        "id": "mYbeSW_DW7Vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Primeramente se encuentra el valor de la media  y la desviacion estandar\n",
        "2.   Seguidamente la Estandarizacion con map\n",
        "3.   Finalmente devolvemos el valor de la Estandarizacion\n",
        "\n"
      ],
      "metadata": {
        "id": "2b1MUw1fZB5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Standardization(df):\n",
        "  #MEDIA\n",
        "  media  = df.mean()\n",
        "  # DESVIACION\n",
        "  stand_deviat = df.stdev()\n",
        "  # CALCULO\n",
        "  standardized =  df.map(lambda A: (A-media)/(stand_deviat))\n",
        "  #DEVOLUCION\n",
        "  return standardized"
      ],
      "metadata": {
        "id": "iEpNv3gwWgqY"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "\n",
        "1.   Generacion de df mediante arreglo de numeros pares\n",
        "2.   Paralelizacion RDD con 4 particiones\n",
        "3.   llamamos a nuestra funcion y la guardamos en una variable Standardiz\n",
        "\n"
      ],
      "metadata": {
        "id": "rTH9M_GwaYBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df=[2,4,6,8,10,12,14,16,18,20,22]\n",
        "Array = sc.parallelize(df,4)\n",
        "standardiz = Standardization(Array)"
      ],
      "metadata": {
        "id": "MRV9JqsfYN0w"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coleccion de standarizacion"
      ],
      "metadata": {
        "id": "LzNKQxQIa4Et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standardiz.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zGEkVdNYvJV",
        "outputId": "8848e53c-f7a1-4aae-9873-bef1a9937f1d"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1.5811388300841895,\n",
              " -1.2649110640673518,\n",
              " -0.9486832980505138,\n",
              " -0.6324555320336759,\n",
              " -0.31622776601683794,\n",
              " 0.0,\n",
              " 0.31622776601683794,\n",
              " 0.6324555320336759,\n",
              " 0.9486832980505138,\n",
              " 1.2649110640673518,\n",
              " 1.5811388300841895]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4) DISTANCIA DE JACCARD**\n",
        "---\n"
      ],
      "metadata": {
        "id": "nUL4xfj8tYk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DISTANCIA JACCARD PARA ELEMENTOS DE UN RDD"
      ],
      "metadata": {
        "id": "6T71hR5QwaNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Primeramente generacion de una lista con zip para los Arrays.\n",
        "2.   Seguidamente se crea una varaible ValRDD para el RDD.\n",
        "3.   Operacion con map para la suma de productos para sum1 y sum2.\n",
        "4.   Division entre ambas variables.\n",
        "5.   Finalmente se le quita a 1 tal division."
      ],
      "metadata": {
        "id": "UBiWvIJSwxBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Jaccard_Distance(A,B):\n",
        "  C = list(zip(A,B))\n",
        "  valRDD = sc.parallelize(C)\n",
        "  sum1 = valRDD.map(lambda A: A[0]*A[1]).sum()\n",
        "  sum2 = valRDD.map(lambda A: 1 if (A[1]==1 or A[0]==1 ) else 0 ).sum()\n",
        "  Djaccard = 1 - (sum1 / sum2)\n",
        "  return Djaccard"
      ],
      "metadata": {
        "id": "ZfL0w0r_tY1k"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos dos array para calcular la distancia\n",
        "\n"
      ],
      "metadata": {
        "id": "Hu7gdx9pzbo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A =[2,4,6,8]\n",
        "B =[10,12,1,16]"
      ],
      "metadata": {
        "id": "pnJnonRjyrXu"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llamamos a a la funcion Jaccard_Distance"
      ],
      "metadata": {
        "id": "2TIGpxyqziug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Jaccard_Distance(A,B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gEbz-LRvTYv",
        "outputId": "76ed25a4-2a71-4b06-f688-83dfdcfd77c3"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-201.0"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4) SIMILITUD DE COSENOS**\n",
        "---\n"
      ],
      "metadata": {
        "id": "HwsxcEUDzpvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIMILITUD DE COSENOS PARA ELEMENTOS DE UN RDD"
      ],
      "metadata": {
        "id": "12tDWc1b2N8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Primeramente tenemos que crear un modulo punto con coordenadas Xi, Yi para luego reducirlos sumando los productos con map.\n",
        "2.   Ahora creamos la funcion cos_similitud donde el numerador que viene a ser A equival al producto punto entre Xi, Yi, y \"B\" quivale al denominador que esta  dado como  el producto pero de vectores normales de Xi, Yi.\n",
        "3.   Devolvemos la division entre A y B (Numerador/Denominador)\n"
      ],
      "metadata": {
        "id": "SdNvlXcf7_e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MODULO PUNTO\n",
        "def Pto(Xi,Yi):\n",
        "        return Xi.zip(Yi).map(lambda Xi: Xi[0]*Xi[1]).reduce(lambda Xi , Yi :Xi+Yi)"
      ],
      "metadata": {
        "id": "ybht__SL6b9m"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODULO SIMILITUD DE COSENOS\n",
        "def Cos_Similitud(Xi,Yi): \n",
        "    A = Pto(Xi,Yi)\n",
        "    B = Pto(Xi,Xi)*punto(Yi,Yi)\n",
        "    return A/B"
      ],
      "metadata": {
        "id": "5DC29NH82OUr"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Para Xi, se tendra valores en un rango de 0-25 y Yi en un rango de 25 a 50 con 4 particiones.\n",
        "2.   Finalmente llamamos a la funcion Cos_Similitud para calcular la similitud de cosenos."
      ],
      "metadata": {
        "id": "S7dmL66C9vus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xi = sc.parallelize(range(0,25),4) \n",
        "Yi = sc.parallelize(range(25, 50),4) \n",
        "print(SimilitudCosenos(Xi,Yi)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0y4i0dc2Rfu",
        "outputId": "b8631fd7-4e60-41f3-8a62-e64b98cbbc68"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.123468669663502e-05\n"
          ]
        }
      ]
    }
  ]
}